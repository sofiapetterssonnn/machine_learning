{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminant Analysis\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import sklearn.discriminant_analysis as skl_da\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.metrics import (accuracy_score,confusion_matrix,f1_score, classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 0.59 MB\n",
      "The first 10 rows in the data:\n"
     ]
    }
   ],
   "source": [
    "#Import data\n",
    "data = pd.read_csv(\"../../machine_learning/siren_data_train.csv\", sep=\",\")\n",
    "print(f\"Data size: {data.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(\"The first 10 rows in the data:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate distance to horn\n",
    "list_loc_horn = []\n",
    "list_loc_person = []\n",
    "list_distance_to_horn = []\n",
    "\n",
    "x_cor_horn = data[\"near_x\"]\n",
    "y_cor_horn = data[\"near_y\"]\n",
    "\n",
    "x_cor_person = data[\"xcoor\"]\n",
    "y_cor_person = data[\"ycoor\"]\n",
    "\n",
    "for row in range(len(x_cor_horn)):\n",
    "    loc_horn = [x_cor_horn[row], y_cor_horn[row]]\n",
    "    list_loc_horn.append(loc_horn)\n",
    "\n",
    "    loc_person = [x_cor_person[row], y_cor_person[row]]\n",
    "    list_loc_person.append(loc_person)\n",
    "\n",
    "for i in range(len(x_cor_horn)):\n",
    "    \n",
    "    coordinate_horn = list_loc_horn[i] \n",
    "    coordinate_person = list_loc_person[i]\n",
    "    distance_to_horn = math.dist(coordinate_horn,coordinate_person)\n",
    "    list_distance_to_horn.append(distance_to_horn)\n",
    "data[\"distance to nearest horn\"] = list_distance_to_horn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" np.random.seed(1)\\ntrainI = np.random.choice(data.shape[0], size=4283, replace=False)\\ntrainIndex = data.index.isin(trainI)\\ntrain = data.iloc[trainIndex] #training data\\ntest = data.iloc[~trainIndex] #test data\\n\\nX_test = train[['building','noise','asleep','in_vehicle','no_windows','age','distance to nearest horn']]\\nY_test = train['heard']\\nX_train = train[['building','noise','asleep','in_vehicle','no_windows','age','distance to nearest horn']]\\nY_train = train['heard'] \""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4499    1\n",
      "3499    1\n",
      "2430    1\n",
      "325     0\n",
      "1967    1\n",
      "       ..\n",
      "2899    1\n",
      "2752    1\n",
      "4363    0\n",
      "2270    1\n",
      "4927    0\n",
      "Name: heard, Length: 1428, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Splitting\n",
    "x = data[['building','noise','asleep','in_vehicle','no_windows','age','distance to nearest horn']]\n",
    "y = data['heard']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 1, shuffle = True)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9124649859943977\n",
      "{'shrinkage': None, 'solver': 'lsqr'}\n",
      "[{'shrinkage': None, 'solver': 'lsqr'}, {'shrinkage': None, 'solver': 'eigen'}, {'shrinkage': 'auto', 'solver': 'lsqr'}, {'shrinkage': 'auto', 'solver': 'eigen'}, {'shrinkage': 0.5, 'solver': 'lsqr'}, {'shrinkage': 0.5, 'solver': 'eigen'}]\n",
      "[0.94031286 0.94031286 0.9373965  0.9373965  0.92311039 0.92311039]\n"
     ]
    }
   ],
   "source": [
    "#Tuning model with GridSearchCV with parameters solver and shrinkage. Solver 'svd' does not support shrinkage and is therefore removed\n",
    "model = skl_da.LinearDiscriminantAnalysis()\n",
    "params = {'solver':['lsqr','eigen'],'shrinkage':[None,'auto',0.5]}\n",
    "grid_search = GridSearchCV(model, params, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "best_params = grid_search.best_params_\n",
    "f1_scores = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "best_gridsearch_model = grid_search.best_estimator_\n",
    "accuracy = best_gridsearch_model.score(X_test, Y_test)\n",
    "print(accuracy)\n",
    "print(best_params)\n",
    "print(params)\n",
    "print(f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9124649859943977\n"
     ]
    }
   ],
   "source": [
    "#Default model\n",
    "model = skl_da.LinearDiscriminantAnalysis()\n",
    "model.fit(X_train, Y_train)\n",
    "accuracy = model.score(X_test, Y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class order in the model:\n",
      "[0 1]\n",
      "Examples of predicted probablities for the above classes:\n",
      "[[0.013 0.987]\n",
      " [0.283 0.717]\n",
      " [0.004 0.996]\n",
      " [0.976 0.024]\n",
      " [0.07  0.93 ]]\n"
     ]
    }
   ],
   "source": [
    "#predict \n",
    "predict_prob = model.predict_proba(X_test)\n",
    "print('The class order in the model:')\n",
    "print(model.classes_)\n",
    "print('Examples of predicted probablities for the above classes:')\n",
    "with np.printoptions(suppress=True, precision=3): \n",
    "    print(predict_prob[0:5]) # First 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five predictions:\n",
      "[1 1 1 0 1] \n",
      "\n",
      "Consufion matrix:\n",
      "heard    0     1\n",
      "row_0           \n",
      "0      223    19\n",
      "1      106  1080 \n",
      "\n",
      "Accuracy: 0.912\n",
      "F1:0.945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.921     0.678     0.781       329\n",
      "           1      0.911     0.983     0.945      1099\n",
      "\n",
      "    accuracy                          0.912      1428\n",
      "   macro avg      0.916     0.830     0.863      1428\n",
      "weighted avg      0.913     0.912     0.907      1428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Evaluate model with confusion matrix\n",
    "prediction = np.empty(len(X_test), dtype=object)\n",
    "prediction = np.where(predict_prob[:, 0]>=0.5, 0, 1)\n",
    "print(\"First five predictions:\")\n",
    "print(prediction[0:5], '\\n') # First 5 predictions after labeling\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Consufion matrix:\")\n",
    "print(pd.crosstab(prediction, Y_test),'\\n')\n",
    "# Accuracy\n",
    "\n",
    "print(f\"Accuracy: {np.mean(prediction == Y_test):.3f}\")\n",
    "print(f\"F1:{f1_score(Y_test,prediction):.3}\")\n",
    "print(classification_report(Y_test,prediction, digits = 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consufion matrix:\n",
      "heard    0     1\n",
      "row_0           \n",
      "1.0    329  1099 \n",
      "\n",
      "Accuracy: 0.770\n",
      "F1:0.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       329\n",
      "           1      0.770     1.000     0.870      1099\n",
      "\n",
      "    accuracy                          0.770      1428\n",
      "   macro avg      0.385     0.500     0.435      1428\n",
      "weighted avg      0.592     0.770     0.669      1428\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vendelalarsson/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vendelalarsson/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vendelalarsson/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "naive = np.ones(Y_test.shape[0])\n",
    "# Confusion matrix\n",
    "print(\"Consufion matrix:\")\n",
    "print(pd.crosstab(naive, Y_test),'\\n')\n",
    "# Accuracy\n",
    "print(f\"Accuracy: {np.mean(naive == Y_test):.3f}\")\n",
    "print(f\"F1:{f1_score(Y_test,naive):.2}\")\n",
    "print(classification_report(Y_test,naive, digits = 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
